{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industry training model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行業部分，我們選用前兩個欄位 k_a08a_1、k_a08a_2 做斷字(單一個字)，當作input的feature，\n",
    "接著建立LSTM模型，調整參數後儲存validation最高的數個模型，存成pickle之後在test預測時使用。參數主要是調整LSTM的unit數和dropout rate。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from openpyxl import load_workbook\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "import numpy as np\n",
    "import jieba\n",
    "import pickle\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wb = load_workbook('train.xlsx')\n",
    "    flag = np.zeros((3167,64))\n",
    "    ws = wb['Sheet1']\n",
    "    cnt = 0\n",
    "    corpus = []\n",
    "    f = open('ind_real.csv')\n",
    "    tmp = f.readlines()\n",
    "    mp = np.zeros((100))\n",
    "    for x in tmp:\n",
    "        s = x.split(',')\n",
    "        id1 = int(s[1].rstrip('\\n'))\n",
    "        if id1>100:\n",
    "            continue\n",
    "        mp[id1] = int(s[0])\n",
    "    for row in ws.rows:\n",
    "        if cnt==0:\n",
    "            cnt +=1\n",
    "            continue\n",
    "        ls = []\n",
    "        tmp_ans = []\n",
    "        for cell in row:\n",
    "            ls.append(cell.value)\n",
    "        ind = int(ls[1])\n",
    "        if ind > 100:\n",
    "            continue\n",
    "        ind = int(mp[ind])\n",
    "        flag[cnt-1][ind] = 1\n",
    "        tmp_list = []\n",
    "        for x in range(len(ls[3])):\n",
    "            tmp_list.append(ls[3][x])\n",
    "        for x in range(len(ls[4])):\n",
    "            tmp_list.append(ls[4][x])\n",
    "\n",
    "        corpus.append(tmp_list)\n",
    "        cnt+=1\n",
    "    MAX_NUM_WORDS = 5000\n",
    "\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    x1_train = []\n",
    "    for x in range(3167):\n",
    "        x1_train.append(tokenizer.texts_to_sequences(corpus[x]))\n",
    "\n",
    "    max_seq_len = max([len(seq) for seq in x1_train])\n",
    "\n",
    "    x_train = np.zeros((3167,40))\n",
    "    for x in range(3167):\n",
    "        for y in range(len(x1_train[x])):\n",
    "            ls = x1_train[x][y]\n",
    "            if len(ls)==0:\n",
    "                continue\n",
    "            x_train[x][y] = ls[0]\n",
    "\n",
    "    NUM_LSTM_UNITS = 256\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(MAX_NUM_WORDS, 1024))\n",
    "    model.add(LSTM(256, dropout=0.1, recurrent_dropout=0.9))\n",
    "#     model.add(Bidirectional(LSTM(256, dropout=0.1, recurrent_dropout=0.5, return_sequences=True)))\n",
    "#     model.add(Bidirectional(LSTM(256, dropout=0.1, recurrent_dropout=0.5, return_sequences=False)))\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    model.add(Dense(units=64, activation='softmax'))\n",
    "\n",
    "    with open('industry_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    rms=keras.optimizers.RMSprop(lr=0.001)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                patience=1,verbose=0,\n",
    "                                mode='auto', min_delta=0.0001,\n",
    "                                cooldown=0, min_lr=0.001)\n",
    "    \n",
    "    model.compile(optimizer=rms,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    earlystopping = EarlyStopping(monitor='val_acc', patience = 30, verbose=1, mode='max') \n",
    "    checkpoint = ModelCheckpoint(filepath='industry2.hdf5', verbose=1, save_best_only=True, \\\n",
    "                                save_weights_only=False, monitor='val_acc', mode='max')\n",
    "    model.summary()\n",
    "    model.fit(x_train,flag,batch_size=128,epochs=300,shuffle=True,verbose=1,validation_split=0.1, callbacks=[checkpoint,earlystopping,reduce_lr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Career training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "職業部分，我們選用後三個欄位 k_a08a_3、k_a08a_4、k_a08a_5 做斷字(單一個字)，當作input的feature，\n",
    "接著建立LSTM模型，調整參數後儲存validation最高的數個模型，存成pickle之後在test預測時使用。參數主要是調整LSTM的unit數和dropout rate。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from openpyxl import load_workbook\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
    "from keras.optimizers import SGD  \n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "import numpy as np\n",
    "import jieba\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    wb = load_workbook('train.xlsx')\n",
    "    flag = np.zeros((3200,120))\n",
    "    ws = wb['Sheet1']\n",
    "    cnt = 0\n",
    "    corpus = []\n",
    "    f = open('job_real.csv')\n",
    "    tmp = f.readlines()\n",
    "    mp = np.zeros((10000,))\n",
    "    for x in tmp:\n",
    "        s = x.split(',')\n",
    "        id1 = int(s[1].rstrip('\\n'))\n",
    "        mp[id1] = int(s[0])\n",
    "    for row in ws.rows:\n",
    "        if cnt==0:\n",
    "            cnt +=1\n",
    "            continue\n",
    "        ls = []\n",
    "        tmp_ans = []\n",
    "        for cell in row:\n",
    "            ls.append(cell.value)\n",
    "\n",
    "        tmp_list = []\n",
    "        ind = int(ls[2])\n",
    "        ind = int(mp[ind])\n",
    "        flag[cnt-1][ind] = 1\n",
    "        for x in range(len(ls[5])):\n",
    "            tmp_list.append(ls[5][x])\n",
    "        for x in range(len(ls[6])):\n",
    "            tmp_list.append(ls[6][x])\n",
    "        for x in range(len(ls[7])):\n",
    "            tmp_list.append(ls[7][x])\n",
    "\n",
    "        corpus.append(tmp_list)\n",
    "        cnt+=1\n",
    "    MAX_NUM_WORDS = 3000\n",
    "\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    x1_train = []\n",
    "    for x in range(3200):\n",
    "        x1_train.append(tokenizer.texts_to_sequences(corpus[x]))\n",
    "\n",
    "    max_seq_len = max([len(seq) for seq in x1_train])\n",
    "    print(max_seq_len)\n",
    "    x_train = np.zeros((3200,65))\n",
    "    for x in range(3200):\n",
    "        for y in range(len(x1_train[x])):\n",
    "            ls = x1_train[x][y]\n",
    "            if len(ls)==0:\n",
    "                continue\n",
    "            x_train[x][y] = ls[0]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(MAX_NUM_WORDS, 1024))\n",
    "#     model.add(LSTM(256, dropout=0.1, recurrent_dropout=0.9))\n",
    "    model.add(Bidirectional(LSTM(256, dropout=0.1, recurrent_dropout=0.5, return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(256, dropout=0.1, recurrent_dropout=0.5, return_sequences=False)))\n",
    "    model.add(Activation('tanh'))\n",
    "\n",
    "    model.add(Dense(units=120, activation='softmax'))\n",
    "\n",
    "    with open('job_tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#     sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    rms=keras.optimizers.RMSprop(lr=0.001)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                                patience=1,verbose=0,\n",
    "                                mode='auto', min_delta=0.0001,\n",
    "                                cooldown=0, min_lr=0.001)\n",
    "    model.compile(optimizer=rms,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    earlystopping = EarlyStopping(monitor='val_acc', patience = 50, verbose=1, mode='max') \n",
    "    checkpoint = ModelCheckpoint(filepath='career2.hdf5', verbose=1, save_best_only=True, \\\n",
    "                                save_weights_only=False, monitor='val_acc', mode='max')\n",
    "    model.summary()\n",
    "    model.fit(x_train,flag,batch_size=128,epochs=300,shuffle=True,verbose=1,validation_split=0.1, callbacks=[checkpoint,earlystopping,reduce_lr])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用前面 train 完後，validation 準確率較高的 10 個模型，利用ensemble方法，給予不同準確率的模型權重，再預測最有可能的職業及行業，結果會儲存在 ml.csv 中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "import pickle\n",
    "import keras\n",
    "from openpyxl import load_workbook\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Activation,LSTM\n",
    "from keras.optimizers import SGD \n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import Input\n",
    "from keras.models import load_model\n",
    "if __name__ == '__main__':\n",
    "    wb = load_workbook('test.xlsx')\n",
    "    with open('job_tokenizer.pickle', 'rb') as handle:\n",
    "        job_tokenizer = pickle.load(handle)\n",
    "    with open('industry_tokenizer.pickle', 'rb') as handle:\n",
    "        industry_tokenizer = pickle.load(handle)\n",
    "    ws = wb['Sheet1']\n",
    "    cnt = 0\n",
    "    ind_model = load_model('industry20.hdf5')\n",
    "    ind_model2 = load_model('industry8.hdf5')\n",
    "    ind_model3 = load_model('industry14.hdf5')\n",
    "    ind_model4 = load_model('industry19.hdf5')\n",
    "    ind_model5 = load_model('industry15.hdf5')\n",
    "    ind_model6 = load_model('industry12.hdf5')\n",
    "    ind_model7 = load_model('industry7.hdf5')\n",
    "    ind_model8 = load_model('industry16.hdf5')\n",
    "    ind_model9 = load_model('industry18.hdf5')\n",
    "    ind_model10 = load_model('industry17.hdf5')\n",
    "\n",
    "    job_model = load_model('career11.hdf5')\n",
    "    job_model2 = load_model('career8.hdf5')\n",
    "    job_model3 = load_model('career9.hdf5')\n",
    "    job_model4 = load_model('career12.hdf5')\n",
    "    job_model5 = load_model('career6.hdf5')\n",
    "    job_model6 = load_model('career10.hdf5')\n",
    "    job_model7 = load_model('career7.hdf5')\n",
    "    job_model8 = load_model('career3.hdf5')\n",
    "    job_model9 = load_model('career2.hdf5')\n",
    "    job_model10 = load_model('career5.hdf5')\n",
    "\n",
    "    ou = open('ml.csv','w')\n",
    "    f = open('job_real.csv','r')\n",
    "    tmp = f.readlines()\n",
    "    mp2 = np.zeros((200,))\n",
    "    for x in tmp:\n",
    "        s = x.split(',')\n",
    "        #print(s)\n",
    "        id1 = int(s[1].rstrip('\\n'))\n",
    "        id0 = int(s[0])\n",
    "        print(id0,id1)\n",
    "        mp2[id0] = id1\n",
    "    mp1 = np.zeros((200,))\n",
    "    f = open('ind_real.csv','r')\n",
    "    tmp = f.readlines()\n",
    "    for x in tmp:\n",
    "        s = x.split(',')\n",
    "        #print(s)\n",
    "        id1 = int(s[1].rstrip('\\n'))\n",
    "        print(id1)\n",
    "\n",
    "        id0 = int(s[0])\n",
    "        mp1[id0] = id1\n",
    "    print('x01,prediction',file=ou)\n",
    "    for row in ws.rows:\n",
    "        if cnt==0:\n",
    "            cnt +=1\n",
    "            continue\n",
    "        ls = []\n",
    "        tmp_ans = []\n",
    "\n",
    "        for cell in row:\n",
    "            ls.append(cell.value)\n",
    "        ind = ls[0]\n",
    "\n",
    "        tmp_list = []\n",
    "        tmp_list2 = []\n",
    "        cut = jieba.cut_for_search(ls[1])\n",
    "        for x in range(len(ls[1])):\n",
    "            tmp_list.append(ls[1][x])\n",
    "        for x in range(len(ls[2])):\n",
    "            tmp_list.append(ls[2][x])\n",
    "        for x in range(len(ls[3])):\n",
    "            tmp_list2.append(ls[3][x])\n",
    "        for x in range(len(ls[4])):\n",
    "            tmp_list2.append(ls[4][x])\n",
    "        for x in range(len(ls[5])):\n",
    "            tmp_list2.append(ls[5][x])\n",
    "        st1 = industry_tokenizer.texts_to_sequences(tmp_list)\n",
    "        st2 = job_tokenizer.texts_to_sequences(tmp_list2)\n",
    "        inp = np.zeros((1,64))\n",
    "        i = 0\n",
    "        for x in st1:\n",
    "            if len(x)!=0:\n",
    "                inp[0,i] = x[0]\n",
    "                i+=1\n",
    "            else:\n",
    "                i+=1\n",
    "        #print(inp)\n",
    "        '''model ensemble'''\n",
    "        p1 = ind_model.predict(inp)\n",
    "        p12 = ind_model2.predict(inp)\n",
    "        p13 = ind_model3.predict(inp)\n",
    "        p14 = ind_model4.predict(inp)\n",
    "        p15 = ind_model5.predict(inp)\n",
    "        p16 = ind_model6.predict(inp)\n",
    "        p17 = ind_model7.predict(inp)\n",
    "        p18 = ind_model8.predict(inp)\n",
    "        p19 = ind_model9.predict(inp)\n",
    "        p10 = ind_model10.predict(inp)\n",
    "        p1=p1+p12+0.8*p13+0.8*p14+0.6*p15+0.6*p16+0.4*p17+0.4*p18+0.2*p19+0.2*p10\n",
    "\n",
    "        res1  = np.argmax(p1)\n",
    "        print(res1)\n",
    "        inp = np.zeros((1,65))\n",
    "        i = 0\n",
    "        for x in st2:\n",
    "            if len(x)!=0:\n",
    "                inp[0,i] = x[0]\n",
    "                i+=1\n",
    "            else:\n",
    "                i+=1\n",
    "        #print(inp)\n",
    "        '''model ensemble'''\n",
    "        p2 = job_model.predict(inp)\n",
    "        p22 = job_model2.predict(inp)\n",
    "        p23 = job_model3.predict(inp)\n",
    "        p24 = job_model4.predict(inp)\n",
    "        p25 = job_model5.predict(inp)\n",
    "        p26 = job_model6.predict(inp)\n",
    "        p27 = job_model7.predict(inp)\n",
    "        p28 = job_model8.predict(inp)\n",
    "        p29 = job_model9.predict(inp)\n",
    "        p20 = job_model10.predict(inp)\n",
    "        p2=p2+p22+0.8*p23+0.8*p24+0.6*p25+0.6*p26+0.4*p27+0.4*p28+0.2*p29+0.2*p20\n",
    "\n",
    "        res2  = np.argmax(p2)\n",
    "        print(res2)\n",
    "        print(str(ind)+'_a08a01,'+str(int(mp1[res1])),file=ou)\n",
    "        print(str(ind)+'_a08a02,'+str(int(mp2[res2])),file=ou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "助教若需要執行，可以只執行test.py這個檔案就好，之前訓練完的模型已經存起來了(.hdf5檔)。training model分別是train_industry.py及train_career.py檔。我們最好的兩次預測結果在ml_good.csv及ml_goooood.csv這兩個檔案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整執行步驟為：train_industry.py > train_career.py > test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
