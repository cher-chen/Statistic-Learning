{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "第一題 [myknn_regressor]\n",
    "首先，載入資料，並將X_train、Y_train、X_test、Y_test分別存入array。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# load data: X_train, Y_train, X_test, Y_test\n",
    "with (open(\"msd_data1.pickle\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            dataset = pickle.load(openfile)        \n",
    "        except EOFError:\n",
    "            break\n",
    "\n",
    "X_train = np.array(dataset['X_train']) # 90*5000 2D-array\n",
    "Y_train = np.array(dataset['Y_train']) # 5000 1D-array\n",
    "X_test = np.array(dataset['X_test']) # 90*3000 2D-array\n",
    "Y_test = np.array(dataset['Y_test']) # 3000 1D-array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myknn_regressor類別，含有以下function:\n",
    "    EuclidianDistance: 傳入兩一維陣列，計算兩個feature陣列的EuclidianDistance\n",
    "    remove_outliers: 移除不在[Q1−1.5IQR,Q3+1.5IQR]的離群值\n",
    "    fit: 傳入X_train, Y_train，做assign\n",
    "    predict: 傳入X_test，計算X_train與每一筆X_test的距離，取最近的K個，並利用這些feature所對應的Y_train之平均做為預測的y      \n",
    "其他function:\n",
    "    RMSE: 傳入兩陣列(預測值及實際值)，計算誤差\n",
    "    Standardize: 傳入陣列做標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# create class myknn_regressor\n",
    "class myknn_regressor:\n",
    "    \n",
    "    def __init__(self, k, method): \n",
    "        self.k = k \n",
    "        self.method = method\n",
    "        self.X_train = []\n",
    "        self.Y_train = []\n",
    "        \n",
    "    # fit the data \n",
    "    def fit(self, X_train, Y_train):\n",
    "        \n",
    "        self.X_train = X_train.copy()\n",
    "        self.Y_train = Y_train.copy()\n",
    "    \n",
    "    # calculate the Euclidian Distance\n",
    "    def EuclidianDistance(self, data1, data2, length):\n",
    "        return np.sqrt(np.sum((data1 - data2) ** 2))\n",
    "        \n",
    "    def remove_outliers(self, data):\n",
    "        \n",
    "        data_removeOutlier = []\n",
    "        IQR = np.quantile(data, 0.75) - np.quantile(data, 0.25)\n",
    "        lb = np.quantile(data, 0.25) - 1.5 * IQR\n",
    "        ub = np.quantile(data, 0.75) + 1.5 * IQR\n",
    "        for i in range(len(data)):\n",
    "            if data[i] >= lb and data[i] <= ub:\n",
    "                data_removeOutlier.append(data[i])\n",
    "                \n",
    "        return data_removeOutlier\n",
    "    \n",
    "        \n",
    "    # predicted y \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        N_test = len(X_test)\n",
    "        pred_y = np.zeros(N_test)\n",
    "        \n",
    "        for s in range(N_test):\n",
    "            distList = []\n",
    "            for i in range(len(self.X_train)): \n",
    "                dist = self.EuclidianDistance(self.X_train[i], X_test[s], len(self.X_train[i]))\n",
    "                distList.append((self.Y_train[i], dist))     \n",
    "            distList.sort(key = lambda x: x[1])\n",
    "            neighbors_y = [distList[i][0] for i in range(self.k)]\n",
    "\n",
    "            if self.method == 'remove_outliers' and self.k >= 10:\n",
    "                neighbors_y = self.remove_outliers(neighbors_y)\n",
    "                pred_y[s] = sum(neighbors_y) / len(neighbors_y)\n",
    "                \n",
    "            else:\n",
    "                pred_y[s] = sum(neighbors_y) / len(neighbors_y)\n",
    "         \n",
    "        return pred_y\n",
    "\n",
    "# Root mean square error\n",
    "def RMSE(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())   \n",
    "\n",
    "# Standardize features\n",
    "def Standardize(data):\n",
    "    \n",
    "    N_feature = len(data[0])\n",
    "    N_data = len(data) \n",
    "    temp = np.ones((N_feature, N_data))\n",
    "    for n in range(N_data):\n",
    "        for f in range(N_feature):\n",
    "            temp[f][n] = data[n][f]\n",
    "    mean = [-1 for n in range(N_feature)]\n",
    "    std = [-1 for n in range(N_feature)]\n",
    "    for f in range(N_feature):\n",
    "        mean[f] = temp[f].mean()\n",
    "        std[f] = temp[f].std()\n",
    "    for n in range(N_data):\n",
    "        for f in range(N_feature):\n",
    "            data[n][f] = (data[n][f] - mean[f]) / std[f] \n",
    "\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_st = Standardize(X_train.copy())\n",
    "X_test_st = Standardize(X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y for equal_weight:\n",
      " [1992.9  1994.05 2000.05 1991.5  1992.8  1998.5  1987.1  1990.9  2001.8\n",
      " 2003.   2001.15 1998.65 1995.55 1997.2  1995.05 1997.35 1992.15 1999.1\n",
      " 2003.6  1995.75]\n",
      "RMSE_equal_weight= 10.292158827638316\n"
     ]
    }
   ],
   "source": [
    "myknn = myknn_regressor(20, \"equal_weight\")\n",
    "myknn.fit(X_train_st, Y_train)\n",
    "ypred = myknn.predict(X_test_st)\n",
    "print('predicted y for equal_weight:\\n',ypred[:20])\n",
    "print('RMSE_equal_weight=', RMSE(ypred, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y for remove_outliers:\n",
      " [1992.9        1994.05       2000.05       1992.73684211 1992.8\n",
      " 2000.         1987.1        1990.9        2001.8        2003.94736842\n",
      " 2001.15       2000.94444444 1995.55       1997.2        1998.61111111\n",
      " 1997.35       1992.15       2004.23529412 2003.6        1995.75      ]\n",
      "RMSE_remove_outliers = 10.225720983625129\n"
     ]
    }
   ],
   "source": [
    "myknn2 = myknn_regressor(20, \"remove_outliers\")\n",
    "myknn2.fit(X_train_st, Y_train)\n",
    "ypred2 = myknn2.predict(X_test_st)\n",
    "print('predicted y for remove_outliers:\\n', ypred2[:20])\n",
    "print('RMSE_remove_outliers =', RMSE(ypred2, Y_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "第二題 [Tuning the Hyper-parameter]\n",
    "\n",
    "利用迴圈對每一個k值做以下處理:\n",
    "Case 1: 使用套件帶入標準化過的training data(即X_train_st)，並輸入標準化後的testing data(X_test_st)輸出預測值ypred，最後和實際值(Y_test)計算誤差(RMSE)，將RMSE存入list中\n",
    "Case 2: 使用套件帶入未標準化過的training data(即X_train)，並輸入未標準化後的testing data(X_test_st)輸出預測值ypred，最後和實際值(Y_test)計算誤差(RMSE)，將RMSE存入list中\n",
    "Case 3: 使用myknn_regressor，方法選擇remove_outliers，並輸入標準化後的testing data(X_test_st)輸出預測值ypred，最後和實際值(Y_test)計算誤差(RMSE)，將RMSE存入list中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    " \n",
    "    \n",
    "Klist = [1,2,3,4,5,10,15,20,25,30,35,40,45,50,55,60,80,100,120,140,160,180,200]\n",
    "\n",
    "RMSE_case1 = []\n",
    "RMSE_case2 = []\n",
    "RMSE_case3 = []\n",
    "\n",
    "for k in Klist:\n",
    "#   case1: knn with standardized feature\n",
    "    case1 = KNeighborsRegressor(n_neighbors = k)\n",
    "    case1.fit(X_train_st, Y_train)\n",
    "    ypred_case1 = case1.predict(X_test_st)\n",
    "    RMSE_case1.append(RMSE(ypred_case1, Y_test))\n",
    "\n",
    "#   case2: knn without standardized feature     \n",
    "    case2 = KNeighborsRegressor(n_neighbors = k)\n",
    "    case2.fit(X_train, Y_train)\n",
    "    ypred_case2 = case2.predict(X_test)\n",
    "    RMSE_case2.append(RMSE(ypred_case2, Y_test))   \n",
    "    \n",
    "#   case3: my knn\n",
    "    myknn = myknn_regressor(k, \"remove_outliers\")\n",
    "    myknn.fit(X_train_st, Y_train)\n",
    "    ypred_case3 = myknn.predict(X_test_st)\n",
    "    RMSE_case3.append(RMSE(ypred_case3, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_case3 = []\n",
    "for k in Klist:\n",
    "    myknn = myknn_regressor(k, \"remove_outliers\")\n",
    "    myknn.fit(X_train_st, Y_train)\n",
    "    ypred_case3 = myknn.predict(X_test_st)\n",
    "    RMSE_case3.append(RMSE(ypred_case3, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "將Case1、Case2、Case3的結果繪製成圖。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RMSE_case1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bba512eea06a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# case3: my knn - green\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMSE_case1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRMSE_case2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#plt.plot(Klist, RMSE_case3, 'g')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RMSE_case1' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# case1: knn with standardized feature - red\n",
    "# case2: knn without standardized feature - blue\n",
    "# case3: my knn - green\n",
    "\n",
    "plt.plot(Klist, RMSE_case1, 'r')\n",
    "plt.plot(Klist, RMSE_case2, 'b')\n",
    "plt.plot(Klist, RMSE_case3, 'g')\n",
    "plt.show()\n",
    "print(RMSE_case1)\n",
    "print(RMSE_case2)\n",
    "print(RMSE_case3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "由上圖可以發現，當k越大時，RMSE越小，即誤差越小，直到某個最低點又很緩慢的回升，呈現U型。\n",
    "先對feature做標準化的結果，RMSE明顯較位做標準化的還低，預測較準確。\n",
    "Case1的誤差最低點發生在k=45，Case2的誤差最低點發生在k=80，而Case3的誤差最低點發生在k=120。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
